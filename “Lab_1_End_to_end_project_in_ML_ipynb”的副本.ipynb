{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZiluJin/Unity_RPGgame_Demo/blob/main/%E2%80%9CLab_1_End_to_end_project_in_ML_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WjuhI5BoMKJ"
      },
      "source": [
        "# Introduction to Jupyter, Python and Pandas and an end-to-end project in Machine Learning\n",
        "\n",
        "Welcome to the Foundations of Machine Learning course. In this course we will introduce the basic concepts of machine learning. In particular we will look at tools and techniques that describe how to model a dataset with the purpose of prediction. An integrated part of that is how we approach data with the computer. We are choosing to do that with the tool you see in front of you: the Jupyter Notebook.\n",
        "\n",
        "Classical software engineering demands a large amount of design and testing. In data analysis, testing remains very important, but the design is often evolving. The design evolves through a process known as *exploratory data analysis*. You will learn some of the techniques of exploratory data analysis in this course.\n",
        "\n",
        "In this Notebook, we will work on an End-to-End project in Machine Learning where we will design a linear regression model to predict bike rentals. Before we go into the project, we will first introduce you to Jupyter, Python and Pandas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX5N0A_3oMKK"
      },
      "source": [
        "### Choice of Language\n",
        "\n",
        "In this module, we will be using Python for our programming language. A prerequisite of attending this course is that you have learnt at least one programming language in the past. It is not our objective to teach you python. At Level 4 and Masters we expect our students to be able pick up a language as they go. If you have not experienced python before it may be worth your while spending some time understanding the language. There are resources available for you to do this [here](https://docs.python.org/3/tutorial/index.html) that are based on the standard console. An introduction to the Jupyter notebook (formerly known as the IPython notebook) is available [here](https://ipython.readthedocs.io/en/stable/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUPvWlALoMKK"
      },
      "source": [
        "### Question 1\n",
        "\n",
        "Who invented python and why? What was the language designed to do? What is the origin of the name \"python\"? Is the language a compiled language? Is it an object orientated language?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDTbtI5-oMKL"
      },
      "source": [
        "Guido van Rossum invented python,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swsVWlJ-oMKL"
      },
      "source": [
        "## Choice of Environment\n",
        "\n",
        "We are working in the Jupyter notebook. It provides an environment for interacting with data in a natural way which is reproducible. We will be learning how to make use of the notebook throughout the course. The notebook allows us to combine code with descriptions, interactive visualizations, plots etc. In fact it allows us to do many of the things we need for data science. Notebooks can also be easily shared through the internet for ease of communication of ideas. The box this text is written in is a *markdown* box. Below we have a *code* box."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtN59stfoMKL",
        "outputId": "e7c7c1d6-cfec-4690-8a73-398d13de0f31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the Jupyter notebook\n",
            "It provides a platform for:\n",
            "Data Open Science\n",
            "Data Science Open\n",
            "Data Open Science\n"
          ]
        }
      ],
      "source": [
        "print(\"This is the Jupyter notebook\")\n",
        "print(\"It provides a platform for:\")\n",
        "words = ['Open', 'Data', 'Science']\n",
        "from random import shuffle\n",
        "for i in range(3):\n",
        "    shuffle(words)\n",
        "    print(' '.join(words))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amlALamZoMKL"
      },
      "source": [
        "Have a play with the code in the above box. Think about the following questions: what is the difference between `CTRL-enter` and `SHIFT-enter` in running the code? What does the command `shuffle` do? Can you find out by typing `shuffle?` in a code box?\n",
        "Once you've had a play with the code we can load in some data using the `pandas` library for data analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shuffle() method takes a sequence, like a list, and reorganize the order of the items.\n",
        "\n",
        "The join() method takes all items in an iterable and joins them into one string."
      ],
      "metadata": {
        "id": "E_NLuXsM2Hvp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdEr6ibcoMKL"
      },
      "source": [
        "## Movie Body Count Example\n",
        "\n",
        "There is a crisis in the movie industry, deaths are occuring on a massive scale. In every feature film the body count is tolling up. But what is the cause of all these deaths? Let's try and investigate.\n",
        "\n",
        "For our first example of data science, we take inspiration from work by [researchers at NJIT](https://www.theswarmlab.com/blog/rvspython/r/2014/02/02/r-vs-python-round-2-2/). The researchers were comparing the qualities of Python with R. They put together a data base of results from the \"Internet Movie Database\" and the [Movie Body Count](http://www.moviebodycounts.com/) website which will allow us to do some preliminary investigation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjlZoJgcoMKM"
      },
      "source": [
        "We will make use of data that has already been 'scraped' from the [Movie Body Count](http://www.moviebodycounts.com/) website. Code and the data is available at [a github repository](https://github.com/sjmgarnier/R-vs-Python/tree/master/Deadliest%20movies%20scrape/code). Git is a version control system and github is a website that hosts code that can be accessed through git. By sharing the code publicly through github, the authors are licensing the code publicly and allowing you to access and edit it. As well as accessing the code via github you can also [download the zip file](https://github.com/sjmgarnier/R-vs-Python/archive/master.zip). But let's do that in python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5T6PBVmoMKM"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve('https://github.com/sjmgarnier/R-vs-Python/archive/master.zip', './master.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4oXSu_foMKM"
      },
      "source": [
        "Once the data is downloaded we can unzip it into the same directory where we are running the lab class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCxby5bLoMKM"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "zip = zipfile.ZipFile('./master.zip', 'r')\n",
        "for name in zip.namelist():\n",
        "    zip.extract(name, '.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xlFOShKoMKM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd # import the pandas library into a namespace called pd\n",
        "film_deaths = pd.read_csv('./R-vs-Python-master/Deadliest movies scrape/code/film-death-counts-Python.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqrE1JcPoMKM"
      },
      "source": [
        "Once it is loaded in the data can be summarized using the `describe` method in pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzCxKBsloMKM"
      },
      "outputs": [],
      "source": [
        "film_deaths.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RsceD0UoMKM"
      },
      "source": [
        "In jupyter and jupyter notebook it is possible to see a list of all possible functions and attributes by typing the name of the object followed by .<Tab> for example in the above case if we type film_deaths.<Tab> it show the columns available (these are attributes in pandas dataframes) such as Body_Count, and also functions, such as .describe().\n",
        "\n",
        "For functions we can also see the documentation about the function by following the name with a question mark. This will open a box with documentation at the bottom which can be closed with the x button."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfw8lBqjoMKM"
      },
      "outputs": [],
      "source": [
        "film_deaths.describe?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LArmXKbdoMKM"
      },
      "outputs": [],
      "source": [
        "print(film_deaths['Year'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ul6PDVAoMKM"
      },
      "source": [
        "This shows the number of deaths per film across the years. We can plot the data as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtpMKhWnoMKM"
      },
      "outputs": [],
      "source": [
        "# this ensures the plot appears in the web browser\n",
        "%matplotlib inline\n",
        "import pylab as plt # this imports the plotting library in python\n",
        "\n",
        "plt.plot(film_deaths['Year'], film_deaths['Body_Count'], 'rx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fko65CqKoMKM"
      },
      "source": [
        "You may be curious what the arguments we give to plt.plot are for, now is the perfect time to look at the documentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVDVeTxEoMKM"
      },
      "outputs": [],
      "source": [
        "plt.plot?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnAEzHOdoMKN"
      },
      "source": [
        "We immediately note that some films have a lot of deaths, which prevent us seeing the detail of the main body of films. First lets identify the films with the most deaths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_F2_CdVoMKN"
      },
      "outputs": [],
      "source": [
        "film_deaths[film_deaths['Body_Count']>200]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfrCQmbUoMKN"
      },
      "source": [
        "Here we are using the command `film_deaths['Body_Count']>200` to index the films in the pandas data frame which have over 200 deaths. To sort them in order we can also use the `sort` command. The result of this command on its own is a data series of `True` and `False` values. However, when it is passed to the `film_deaths` data frame it returns a new data frame which contains only those values for which the data series is `True`. We can also sort the result. To sort the result by the values in the `Body_Count` column in *descending* order we use the following command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2yZe7GPoMKN"
      },
      "outputs": [],
      "source": [
        "film_deaths[film_deaths['Body_Count']>200].sort_values('Body_Count', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ediXur3WoMKN"
      },
      "source": [
        "We now see that the 'Lord of the Rings' is a large outlier with a very large number of kills. We can try and determine how much of an outlier by histograming the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqEZmg4toMKN"
      },
      "source": [
        "### Plotting the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tC9wxJgpoMKN"
      },
      "outputs": [],
      "source": [
        "film_deaths['Body_Count'].hist(bins=20) # histogram the data with 20 bins.\n",
        "plt.title('Histogram of Film Kill Count')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahIXxR0VoMKN"
      },
      "source": [
        "### Question 2\n",
        "Read on the internet about the following python libraries: `numpy`, `matplotlib`, `scipy` and `pandas`. What functionality does each provide in python. What is the `pylab` library and how does it relate to the other libraries?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzWTHRiQoMKN"
      },
      "source": [
        "*Provide your answer here*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgQOCz5boMKN"
      },
      "source": [
        "We could try and remove these outliers, but another approach would be to plot the logarithm of the counts against the year."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_ZJL7R6oMKN"
      },
      "outputs": [],
      "source": [
        "plt.plot(film_deaths['Year'], film_deaths['Body_Count'], 'rx')\n",
        "ax = plt.gca() # obtain a handle to the current axis\n",
        "ax.set_yscale('log') # use a logarithmic death scale\n",
        "# give the plot some titles and labels\n",
        "plt.title('Film Deaths against Year')\n",
        "plt.ylabel('deaths')\n",
        "plt.xlabel('year')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_XlSmyroMKN"
      },
      "source": [
        "Note a few things. We are interacting with our data. In particular, we are replotting the data according to what we have learned so far. We are using the programming language as a *scripting* language to give the computer one command or another, and then the next command we enter is dependent on the result of the previous. This is a very different paradigm to classical software engineering. In classical software engineering we normally write many lines of code (entire object classes or functions) before compiling the code and running it. Our approach is more similar to the approach we take whilst debugging. Historically, researchers interacted with data using a *console*. A command line window which allowed command entry. The notebook format we are using is slightly different. Each of the code entry boxes acts like a separate console window. We can move up and down the notebook and run each part in a different order. The *state* of the program is always as we left it after running the previous part.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdNOZMS4oMKN"
      },
      "source": [
        "### Question 3\n",
        "\n",
        "What is jupyter and why was it invented? Give some examples of functionality it gives over standard python. What is the jupyter project? Name two languages involved in the Jupyter project other than python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmxuVzU9oMKN"
      },
      "source": [
        "*Provide your answer here*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wihf2P-oMKN"
      },
      "source": [
        "# An end-to-end project in ML: using machine learning to predict bike rentals\n",
        "\n",
        "We will know move on and go in detail through an end-to-end ML project. Our dataset comes from a popular machine learning repository that hosts open source datasets for educational and research purposes, the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). We are going to follow several of the steps in the ML project checklist and use several utilities and models in [scikit-learn](https://scikit-learn.org/stable/) for predicting bike rentals. The description of the dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/Seoul+Bike+Sharing+Demand#).\n",
        "\n",
        "### Get the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgedfksDoMKN"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve('https://archive.ics.uci.edu/ml/machine-learning-databases/00560/SeoulBikeData.csv', './SeoulBikeData.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIu9zJQCoMKN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "bike_sharing_data = pd.read_csv('SeoulBikeData.csv', encoding= 'unicode_escape')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmX15FkEoMKQ"
      },
      "source": [
        "We can get a description of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoG-lMiWoMKQ"
      },
      "outputs": [],
      "source": [
        "bike_sharing_data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-phjB5jOoMKQ"
      },
      "source": [
        "We can see some of the rows in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "decMSBJVoMKQ"
      },
      "outputs": [],
      "source": [
        "bike_sharing_data.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnzUpUFJoMKQ"
      },
      "source": [
        "The target variable $y$ corresponds to the Rented Bike Count variable of the second column. The following columns correspond to the variables in the feature vector $\\mathbf{x}$, *e.g.*, *Hour* is $x_1$ up until *Functioning Day* which is $x_D$. The original dataset also has a date column that we are not going to use in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a_l4HujoMKQ"
      },
      "outputs": [],
      "source": [
        "bike_sharing_data = bike_sharing_data.drop('Date', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1HUh5FHoMKQ"
      },
      "source": [
        "We follow some of the steps in the ML checklist we used in the lecture, including data exploration, data preprocessing, and fine-tuning the ML model. It is important to remember that the testing data that we use for assessing the generalisation performance has to be set aside once we get the data. Also, any data preprocessing that you do has to be done only on the training data and several quantities need to be saved for the test stage. Separating the dataset into training and test before any preprocessing has happened, help us to recreate the real world scenario where we will deploy our system and for which the data will come without any preprocessing.\n",
        "\n",
        "We use scikit-learn to separate the data into training and test sets. Let us first look at how many instances we have in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAoog4hfoMKQ"
      },
      "outputs": [],
      "source": [
        "bike_sharing_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZnigDZKoMKQ"
      },
      "source": [
        "Several algorithms that we will use assume the inputs to be type 'float' instead of 'int', so we transform those variables in the dataset from int64 to float64."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhZOCgDLoMKQ"
      },
      "outputs": [],
      "source": [
        "for col in ['Rented Bike Count', 'Hour', 'Humidity(%)', 'Visibility (10m)']:\n",
        "    bike_sharing_data[col] = bike_sharing_data[col].astype('float64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gK0B9wloMKR"
      },
      "outputs": [],
      "source": [
        "bike_sharing_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtSZbfzRoMKR"
      },
      "source": [
        "The dataset has a few thousand observations. We will use 85% of the data for training and 15% for testing. The `train_test_split` function in scikit-learn allows to easily get these partitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksRYpmI0oMKR"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "bs_train_set, bs_test_set = train_test_split(bike_sharing_data, test_size=0.15, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5rRiZb5oMKR"
      },
      "source": [
        "The train and test sets are chosen randomly from all the available data. By specifying a value for `random_state`, we are making sure that every time we run this instruction, the train and test set will have the exact same instances. `random_state` \"controls the shuffling applied to the data before applying the split\".\n",
        "\n",
        "### Explore the data\n",
        "\n",
        "There are different tools we can use to explore the dataset.\n",
        "\n",
        "#### Histograms\n",
        "\n",
        "Let us first look at histograms for each of the continuous attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02AwcgkSoMKR"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "bs_train_set.hist(bins=50, figsize=(20,15))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc_OzjYUoMKR"
      },
      "source": [
        "Some observations from the histograms are:\n",
        "\n",
        "1. The values for the variables Rainfall, Snowfall, Solar Radition and Visibility are concentrated at one of the ends of the plots. This is an indication that several instances might contain outliers. One can consider removing these outliers from the data or binning the data into a few discrete values.\n",
        "\n",
        "2. Both the Rented Bike Count and the Wind Speed are [skewed to the right](https://en.wikipedia.org/wiki/Skewness), this is, the mean of the distribution is to the right of the median. Some ML algorithms find it harder to detect patterns for this type of distribution. One might consider transforming these features using $\\log(x)$ or $\\sqrt{x}$ so that they look more like a bell-shaped distribution.\n",
        "\n",
        "### Question 4\n",
        "\n",
        "1. Compute the mean and the median for the variables Rented Bike Count and Wind Speed and verify that the mean is to the right of the median.\n",
        "\n",
        "2. How would the histograms for Rented Bike Count and the Wind Speed look like if we transform the values using $\\sqrt{x}$?\n",
        "\n",
        "3. Would it be possible to use $\\log{x}$ instead of $\\sqrt{x}$? If not, what would you do to the variable to be able to use it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xe9vk675oMKR"
      },
      "outputs": [],
      "source": [
        "# Provide your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEhiQG6AoMKR"
      },
      "source": [
        "#### Scatter plots\n",
        "\n",
        "The Scatter plot is a tool we can use to explore dependencies between the different variables. It contains plots of each variable against each other in the dataset. If there are many variables in the feature vector, including all scatter plots might not be convenient to visualise. Let us look at the scatter plot for the target variable and four of the attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "sHG3a2oVoMKR"
      },
      "outputs": [],
      "source": [
        "from pandas.plotting import scatter_matrix\n",
        "attributes = ['Rented Bike Count', 'Hour', 'Temperature(°C)', 'Humidity(%)', 'Wind speed (m/s)']\n",
        "figscat = scatter_matrix(bs_train_set[attributes], figsize=(20, 15))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7TkRTgKoMKR"
      },
      "source": [
        "The variables Hour and Temperature seem well correlatd with Rented Bike Count. The relationship between Humidity and Wind Speed with Rented Bike Count looks less clear though.\n",
        "\n",
        "### Correlation coefficients\n",
        "\n",
        "Additionally, we can study the correlation coefficient between the numerical attributes and the Rented Bike Count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5VZ3zMQoMKR"
      },
      "outputs": [],
      "source": [
        "corr_matrix = bs_train_set.corr(numeric_only=True)\n",
        "corr_matrix['Rented Bike Count'].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G41SrstWoMKR"
      },
      "source": [
        "As we suspected by having looked at the scatter plots, Temperature and Hour are strongly correlated with the target value.\n",
        "\n",
        "### Question 5\n",
        "\n",
        "What would the correlation coefficients be if the variables Rented Bike Count and Wind Speed are transformed using $\\sqrt{x}$?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeBZYw0uoMKR"
      },
      "outputs": [],
      "source": [
        "# Provide your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTlK58EgoMKR"
      },
      "source": [
        "### Prepare the data\n",
        "\n",
        "We will now prepare the data so that it is suitable for the machine learning models. We consider the following processes for the dataset in this notebook: using one-hot-encoding for the categorical attributes and feature scaling for the numerical attributes. scikit-learn provides utilities for these tasks:\n",
        "\n",
        "1. [OneHotEncoder()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html?highlight=onehotencoder#sklearn.preprocessing.OneHotEncoder) allows to transform a categorical variable to a one-hot encoding representation.\n",
        "\n",
        "2. [StandardScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?highlight=standardscaler#sklearn.preprocessing.StandardScaler) performs feature scaling by standardisation.\n",
        "\n",
        "`OneHotEncoder()` and `StandardScaler()` are part of the scikit-learn [preprocessing module](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing).\n",
        "\n",
        "### Question 6\n",
        "\n",
        "Explore the scikit-learn [preprocessing module](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing). List and explain two of the utilities availaible that you believe are useful for data analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HxIIQd8oMKR"
      },
      "source": [
        "*Provide your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IVn-oPfoMKR"
      },
      "source": [
        "`OneHotEncoder()` and `StandardScaler()` are examples of [data transformations](https://scikit-learn.org/stable/data_transforms.html). In scikit-learn these are referred to as *transformers* and they map the data from one format to another. In a programming context, transformers are classes. They come with the following methods:\n",
        "\n",
        "- `fit` that is used to learn the  transformation from data.\n",
        "- `transform` that is used to transform the data once the transformer has been fitted.   \n",
        "- `fit_transform` that applies first `fit` and then `transform` to the data.\n",
        "\n",
        "Typically, we use either `fit` or `fit_transform` for the training data and `transform` for the validation or test data.\n",
        "\n",
        "Since the one-hot-encoding and standardisation transformations will be applied to different columns in the dataset, it is convenient to have a function that applies them both. It is particularly useful since we need to apply those transformations to the train, validation and test sets. We could code such function from scratch, but here, we make use of the [ColumnTransformer()](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html?highlight=columntransformer#sklearn.compose.ColumnTransformer), an estimator available in scikit-learn that allows to group different transformations into a single method. `ColumnTransformer` is an example of an *estimator* in scikit-learn. An estimator is an object that provides predictions for new data. Besides the `fit`, `transform` and `fit_transform` methods, they most of the times provide the additional `predict` method for making predictions on the test data.\n",
        "\n",
        "### Question 7\n",
        "\n",
        "A [pipeline](https://scikit-learn.org/stable/modules/compose.html#pipeline) is a very convenient estimator in scikit-learn. Explain what is a pipeline and describe in which situations it is useful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWYn0ZZOoMKR"
      },
      "source": [
        "*Provide your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt-Vt2tZoMKR"
      },
      "source": [
        "Let us go back to the dataset and apply the transformations we mentioned before. We first define lists with the names of the attributes, a list for the categorical attributes and a list for the numerical attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buVAXg-1oMKR"
      },
      "outputs": [],
      "source": [
        "attributes_cat = ['Seasons', 'Holiday', 'Functioning Day']\n",
        "attributes_num = ['Hour', 'Temperature(°C)', 'Humidity(%)', 'Wind speed (m/s)', 'Visibility (10m)', \\\n",
        "                  'Dew point temperature(°C)', 'Solar Radiation (MJ/m2)', 'Rainfall(mm)', 'Snowfall (cm)']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPzid449oMKR"
      },
      "source": [
        "We now import `OneHotEncoder`, `StandardScaler` and `ColumnTransformer` and create the actual transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XFk7xHgoMKR"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "full_transform = ColumnTransformer([\n",
        "    (\"num\", StandardScaler(), attributes_num),\n",
        "    (\"cat\", OneHotEncoder(), attributes_cat),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQeNgyawoMKR"
      },
      "source": [
        "Before applying the full transformation, we separate the target feature from the attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vM9UT3AHoMKS"
      },
      "outputs": [],
      "source": [
        "bs_train_set_attributes = bs_train_set.drop('Rented Bike Count', axis=1)\n",
        "bs_train_set_labels = bs_train_set['Rented Bike Count']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbqVzB_GoMKS"
      },
      "source": [
        "We can now apply the fit and apply the full transformation to the training data using `fit_transform`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVqcJlQHoMKS"
      },
      "outputs": [],
      "source": [
        "bs_train_set_attributes_prepared = full_transform.fit_transform(bs_train_set_attributes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd-0sGkcoMKS"
      },
      "source": [
        "### Short-list models and fine-tune them\n",
        "\n",
        "Up until this point, we have managed to prepare the data so that it can be used for fitting a predictive model. Scikit-learn includes routines for [several different predictive models for regression and for classification](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning). The application of these methods follows a similar template to the point that applying a method intead of other is just a matter of changing the name of the method.\n",
        "\n",
        "In this notebook, we will focus on Linear Regression but in a larger ML project, you will be encouraged to try different predictive models, e.g. from those available in scikit-learn, and short-list two to three that look promising.  \n",
        "\n",
        "We import the [LinearRegression()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html?highlight=linearregression#sklearn.linear_model.LinearRegression) method and fit it to the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R52wkUOuoMKS"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(bs_train_set_attributes_prepared, bs_train_set_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgaBdRBxoMKS"
      },
      "source": [
        "And that's it! We have fit the ML model. What's next? Well, by now, one may feel tempted to apply the model to the test data to see how it performs. However, one should only do this when being absolutely sure that this is the best performing model on a *validation set*.\n",
        "\n",
        "We have not used a validation set up until this point because we have not needed to compare between two alternative models. To see how to fine-tune the model, *let us use a validation set to decide whether including the features Rainfall and Snowfall has any benefits*\n",
        "\n",
        "#### Fine-tuning the model\n",
        "\n",
        "We take the original train set and split it again into a train set and a validation set. Due to the size of the dataset, we use a simple way for validating the model known as *holdout validation*, for which we hold out a single set of data for validation purposes. When the dataset is smaller, you can perform k-fold cross-validation or if the data is really small, leave-one-out cross validation. Both are implemented in scikit-learn ([k-fold cross validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold) and [leave-one-out cross validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html?highlight=leave%20one%20out#sklearn.model_selection.LeaveOneOut).)\n",
        "\n",
        "From the original train set, we used 85% for the train set and 15% for the validation set. We could have split the dataset from the beginning into a train set, a validation set and a test set using 70%, 15% and 15% of the available data, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzCitwDCoMKS"
      },
      "outputs": [],
      "source": [
        "bs_train2_set, bs_val_set = train_test_split(bs_train_set, test_size=0.15, random_state=42)\n",
        "bs_train2_set_attributes = bs_train2_set.drop('Rented Bike Count', axis=1)\n",
        "bs_train2_set_labels = bs_train2_set['Rented Bike Count']\n",
        "bs_val_set_attributes = bs_val_set.drop('Rented Bike Count', axis=1)\n",
        "bs_val_set_labels = bs_val_set['Rented Bike Count']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYIS4wL9oMKS"
      },
      "source": [
        "We will be comparing between two transformations, the one we already described with `full_transform` and one that looks similar except from not including Rainfall and Snowfall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqdCOz8toMKS"
      },
      "outputs": [],
      "source": [
        "attributes_num_partial = ['Hour', 'Temperature(°C)', 'Humidity(%)', 'Wind speed (m/s)', 'Visibility (10m)', \\\n",
        "                  'Dew point temperature(°C)', 'Solar Radiation (MJ/m2)']\n",
        "partial_transform = ColumnTransformer([\n",
        "    (\"num\", StandardScaler(), attributes_num_partial),\n",
        "    (\"cat\", OneHotEncoder(), attributes_cat),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy8am4fUoMKS"
      },
      "source": [
        "We now use this new transformation to fit_transform the new train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_eDwNuboMKS"
      },
      "outputs": [],
      "source": [
        "bs_train2_set_no_RS_attributes = partial_transform.fit_transform(bs_train2_set_attributes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxK7kXbKoMKS"
      },
      "source": [
        "We now train the linear regression model that only uses the partial transformed attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8L2BXO3XoMKS"
      },
      "outputs": [],
      "source": [
        "lin_reg_mod = LinearRegression()\n",
        "lin_reg_mod.fit(bs_train2_set_no_RS_attributes, bs_train2_set_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpADRI_xoMKS"
      },
      "source": [
        "Let us now assess the performance of this model over the validation data. We first need to prepare the validation input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03a_tsLroMKS"
      },
      "outputs": [],
      "source": [
        "bs_val_set_no_RS_attributes = partial_transform.transform(bs_val_set_attributes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4mZ1JohoMKS"
      },
      "source": [
        "We now compute the predictions made by the linear model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7Ev8HbioMKS"
      },
      "outputs": [],
      "source": [
        "bs_val_set_predictions_mod = lin_reg_mod.predict(bs_val_set_no_RS_attributes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0arTXkjoMKS"
      },
      "source": [
        "We can now compute the RMSE obtained with this predictive model. We can use the [scikit-learn routine for computing the mean squared error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error) and then compute the square root."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUCIn5OHoMKS"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "error_mod = np.sqrt(mean_squared_error(bs_val_set_labels, bs_val_set_predictions_mod))\n",
        "error_mod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNT-jTyEoMKS"
      },
      "source": [
        "Let us now look into using all the numerical attributes. The train set has changed, so we need to fit_transform a new full transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAW5JakOoMKS"
      },
      "outputs": [],
      "source": [
        "bs_train2_set_all_attributes = full_transform.fit_transform(bs_train2_set_attributes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85QMQsMhoMKS"
      },
      "source": [
        "We creat the new linear regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "y57Omu58oMKS"
      },
      "outputs": [],
      "source": [
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(bs_train2_set_all_attributes, bs_train2_set_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxtxs1SxoMKS"
      },
      "source": [
        "Transform the validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEJNTkizoMKS"
      },
      "outputs": [],
      "source": [
        "bs_val_set_all_attributes = full_transform.transform(bs_val_set_attributes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWzovZdroMKT"
      },
      "source": [
        "We finally perform the prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8Q3kVQXoMKT"
      },
      "outputs": [],
      "source": [
        "bs_val_set_predictions = lin_reg.predict(bs_val_set_all_attributes)\n",
        "error = np.sqrt(mean_squared_error(bs_val_set_labels, bs_val_set_predictions))\n",
        "error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6iXaU5poMKT"
      },
      "source": [
        "We conclude from this that the variables Rainfall and Snowfall actually help to slightly improve the predictions.\n",
        "\n",
        "### Question 8\n",
        "\n",
        "Perhaps other transformations to the dataset can help to improve the predictions. Try the following transformations and see whether the RMSE over the validation set reduces even more:\n",
        "\n",
        "1. Before standardising the feature Wind speed, first transform it using $\\sqrt{x}$.\n",
        "2. Transform the Rainfall and the Snowfall to discrete features using the scikit-learn utility [KBinsDiscretizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html#sklearn.preprocessing.KBinsDiscretizer) with $K=5$.\n",
        "3. Instead of doing standardisation over the other numerical features, use normalisation.\n",
        "4. Keep the one-hot-encoding for the categorical features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0DuRFCHoMKT"
      },
      "outputs": [],
      "source": [
        "# Provide your answer bere"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHLuSRNcoMKT"
      },
      "source": [
        "Between chosing to include Rainfall and Snowfall or not, the stage of validation tells us we should include them. If this was the only hyperparameter to choose from, we would be done and we could proceed to compute the generalisation error on the test set. Since we are not considering more fine-tuning at the moment, let us compute the RMSE over the test set. We have already prepared the whole training data (what we called train2+val) before using the full transform, we called it `bs_train_set_attributes_prepared`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W743hIvkoMKT"
      },
      "outputs": [],
      "source": [
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(bs_train_set_attributes_prepared, bs_train_set_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkPeMfWYoMKT"
      },
      "source": [
        "Let us transform the test data so that we can apply the fitted model correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IF2g0GxoMKT"
      },
      "outputs": [],
      "source": [
        "bs_test_set_attributes = bs_test_set.drop('Rented Bike Count', axis=1)\n",
        "bs_test_set_labels = bs_test_set['Rented Bike Count']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlSK_KlBoMKT"
      },
      "source": [
        "We now transform the attributes in the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oa3B2zu6oMKT"
      },
      "outputs": [],
      "source": [
        "bs_test_set_attributes_prepared = full_transform.transform(bs_test_set_attributes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2EpAj6XoMKT"
      },
      "source": [
        "We perform the prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZQqQzWPoMKT"
      },
      "outputs": [],
      "source": [
        "bs_test_set_predictions = lin_reg.predict(bs_test_set_attributes_prepared)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hT5fMNHjoMKT"
      },
      "source": [
        "And compute the RMSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkmRi8B_oMKT"
      },
      "outputs": [],
      "source": [
        "error_test = np.sqrt(mean_squared_error(bs_test_set_labels, bs_test_set_predictions))\n",
        "error_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzoCs_BGoMKT"
      },
      "source": [
        "The performance in the test set is slightly worse when compared to the performance in the validation set."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}